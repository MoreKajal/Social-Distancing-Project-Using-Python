{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SocialDistance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1V7HdkV5afXsY75biJspD4RYmQ3syX6qV",
      "authorship_tag": "ABX9TyO3LoBDccbTIDIFPOf1nDe3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoreKajal/Social-Distancing-Project-Using-Python/blob/main/SocialDistance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9uiZvXr0LQm"
      },
      "source": [
        "\r\n",
        "# **Setting up the variable values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbd60rIo0Cvi"
      },
      "source": [
        "# initialize minimum probability to filter weak detections along with\r\n",
        "# the threshold when applying non-maxima suppression\r\n",
        "MIN_CONF = 0.3\r\n",
        "NMS_THRESH = 0.3\r\n",
        "\r\n",
        "# define the minimum safe distance (in pixels) that two people can be\r\n",
        "# from each other\r\n",
        "MIN_DISTANCE = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVVISvod0ZNe"
      },
      "source": [
        "# **Creating the People Detection Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty2em52a0cWc"
      },
      "source": [
        "# import the necessary packages\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "\r\n",
        "def detect_people(frame, net, ln, personIdx=0):\r\n",
        "\t# grab the dimensions of the frame and  initialize the list of\r\n",
        "\t# results\r\n",
        "\t(H, W) = frame.shape[:2]\r\n",
        "\tresults = []\r\n",
        "\r\n",
        "\t# construct a blob from the input frame and then perform a forward\r\n",
        "\t# pass of the YOLO object detector, giving us our bounding boxes\r\n",
        "\t# and associated probabilities\r\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\r\n",
        "\t\tswapRB=True, crop=False)\r\n",
        "\tnet.setInput(blob)\r\n",
        "\tlayerOutputs = net.forward(ln)\r\n",
        "\r\n",
        "\t# initialize our lists of detected bounding boxes, centroids, and\r\n",
        "\t# confidences, respectively\r\n",
        "\tboxes = []\r\n",
        "\tcentroids = []\r\n",
        "\tconfidences = []\r\n",
        "\r\n",
        "\t# loop over each of the layer outputs\r\n",
        "\tfor output in layerOutputs:\r\n",
        "\t\t# loop over each of the detections\r\n",
        "\t\tfor detection in output:\r\n",
        "\t\t\t# extract the class ID and confidence (i.e., probability)\r\n",
        "\t\t\t# of the current object detection\r\n",
        "\t\t\tscores = detection[5:]\r\n",
        "\t\t\tclassID = np.argmax(scores)\r\n",
        "\t\t\tconfidence = scores[classID]\r\n",
        "\r\n",
        "\t\t\t# filter detections by (1) ensuring that the object\r\n",
        "\t\t\t# detected was a person and (2) that the minimum\r\n",
        "\t\t\t# confidence is met\r\n",
        "\t\t\tif classID == personIdx and confidence > MIN_CONF:\r\n",
        "\t\t\t\t# scale the bounding box coordinates back relative to\r\n",
        "\t\t\t\t# the size of the image, keeping in mind that YOLO\r\n",
        "\t\t\t\t# actually returns the center (x, y)-coordinates of\r\n",
        "\t\t\t\t# the bounding box followed by the boxes' width and\r\n",
        "\t\t\t\t# height\r\n",
        "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\r\n",
        "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\r\n",
        "\r\n",
        "\t\t\t\t# use the center (x, y)-coordinates to derive the top\r\n",
        "\t\t\t\t# and and left corner of the bounding box\r\n",
        "\t\t\t\tx = int(centerX - (width / 2))\r\n",
        "\t\t\t\ty = int(centerY - (height / 2))\r\n",
        "\r\n",
        "\t\t\t\t# update our list of bounding box coordinates,\r\n",
        "\t\t\t\t# centroids, and confidences\r\n",
        "\t\t\t\tboxes.append([x, y, int(width), int(height)])\r\n",
        "\t\t\t\tcentroids.append((centerX, centerY))\r\n",
        "\t\t\t\tconfidences.append(float(confidence))\r\n",
        "\r\n",
        "\t# apply non-maxima suppression to suppress weak, overlapping\r\n",
        "\t# bounding boxes\r\n",
        "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\t# ensure at least one detection exists\r\n",
        "\tif len(idxs) > 0:\r\n",
        "\t\t# loop over the indexes we are keeping\r\n",
        "\t\tfor i in idxs.flatten():\r\n",
        "\t\t\t# extract the bounding box coordinates\r\n",
        "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\r\n",
        "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\r\n",
        "\r\n",
        "\t\t\t# update our results list to consist of the person\r\n",
        "\t\t\t# prediction probability, bounding box coordinates,\r\n",
        "\t\t\t# and the centroid\r\n",
        "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\r\n",
        "\t\t\tresults.append(r)\r\n",
        "\r\n",
        "\t# return the list of results\r\n",
        "\treturn results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW8jdmQe0mVB"
      },
      "source": [
        "# **Grab frames from video and make prediction measuring distances of detected people**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imsw8E9z0vmD"
      },
      "source": [
        "# USAGE\r\n",
        "# python social_distance_detector.py --input pedestrians.mp4\r\n",
        "# python social_distance_detector.py --input pedestrians.mp4 --output output.avi\r\n",
        "\r\n",
        "# import the necessary packages\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "from scipy.spatial import distance as dist\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import imutils\r\n",
        "import cv2\r\n",
        "import os\r\n",
        "\r\n",
        "# construct the argument parse and parse the arguments\r\n",
        "ap = argparse.ArgumentParser()\r\n",
        "ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\r\n",
        "\thelp=\"path to (optional) input video file\")\r\n",
        "ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\r\n",
        "\thelp=\"path to (optional) output video file\")\r\n",
        "ap.add_argument(\"-d\", \"--display\", type=int, default=1,\r\n",
        "\thelp=\"whether or not output frame should be displayed\")\r\n",
        "args = vars(ap.parse_args([\"--input\",\"/content/drive/MyDrive/M.Tech Projects/Social-distance-detection/pedestrians.mp4\",\r\n",
        "                           \"--output\",\"/content/drive/MyDrive/M.Tech Projects/Social-distance-detection/MyCodes/my_output.avi\",\r\n",
        "                           \"--display\",\"1\"]))\r\n",
        "\r\n",
        "# load the COCO class labels our YOLO model was trained on\r\n",
        "labelsPath = os.path.sep.join([\"/content/drive/MyDrive/M.Tech Projects/Social-distance-detection/yolo-coco_files/coco.names\"])\r\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\")\r\n",
        "\r\n",
        "# derive the paths to the YOLO weights and model configuration\r\n",
        "weightsPath = os.path.sep.join([\"/content/drive/MyDrive/M.Tech Projects/Social-distance-detection/yolo-coco_files/yolov3.weights\"])\r\n",
        "configPath = os.path.sep.join([\"/content/drive/MyDrive/M.Tech Projects/Social-distance-detection/yolo-coco_files/yolov3.cfg\"])\r\n",
        "\r\n",
        "# load our YOLO object detector trained on COCO dataset (80 classes)\r\n",
        "print(\"[INFO] loading YOLO from disk...\")\r\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\r\n",
        "\r\n",
        "# determine only the *output* layer names that we need from YOLO\r\n",
        "ln = net.getLayerNames()\r\n",
        "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\r\n",
        "\r\n",
        "# initialize the video stream and pointer to output video file\r\n",
        "print(\"[INFO] accessing video stream...\")\r\n",
        "vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\r\n",
        "writer = None\r\n",
        "\r\n",
        "# loop over the frames from the video stream\r\n",
        "while True:\r\n",
        "\t# read the next frame from the file\r\n",
        "\t(grabbed, frame) = vs.read()\r\n",
        "\r\n",
        "\t# if the frame was not grabbed, then we have reached the end\r\n",
        "\t# of the stream\r\n",
        "\tif not grabbed:\r\n",
        "\t\tbreak\r\n",
        "\r\n",
        "\t# resize the frame and then detect people (and only people) in it\r\n",
        "\tframe = imutils.resize(frame, width=700)\r\n",
        "\tresults = detect_people(frame, net, ln,\r\n",
        "\t\tpersonIdx=LABELS.index(\"person\"))\r\n",
        "\r\n",
        "\t# initialize the set of indexes that violate the minimum social\r\n",
        "\t# distance\r\n",
        "\tviolate = set()\r\n",
        "\r\n",
        "\t# ensure there are *at least* two people detections (required in\r\n",
        "\t# order to compute our pairwise distance maps)\r\n",
        "\tif len(results) >= 2:\r\n",
        "\t\t# extract all centroids from the results and compute the\r\n",
        "\t\t# Euclidean distances between all pairs of the centroids\r\n",
        "\t\tcentroids = np.array([r[2] for r in results])\r\n",
        "\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\r\n",
        "\r\n",
        "\t\t# loop over the upper triangular of the distance matrix\r\n",
        "\t\tfor i in range(0, D.shape[0]):\r\n",
        "\t\t\tfor j in range(i + 1, D.shape[1]):\r\n",
        "\t\t\t\t# check to see if the distance between any two\r\n",
        "\t\t\t\t# centroid pairs is less than the configured number\r\n",
        "\t\t\t\t# of pixels\r\n",
        "\t\t\t\tif D[i, j] < MIN_DISTANCE:\r\n",
        "\t\t\t\t\t# update our violation set with the indexes of\r\n",
        "\t\t\t\t\t# the centroid pairs\r\n",
        "\t\t\t\t\tviolate.add(i)\r\n",
        "\t\t\t\t\tviolate.add(j)\r\n",
        "\r\n",
        "\t# loop over the results\r\n",
        "\tfor (i, (prob, bbox, centroid)) in enumerate(results):\r\n",
        "\t\t# extract the bounding box and centroid coordinates, then\r\n",
        "\t\t# initialize the color of the annotation\r\n",
        "\t\t(startX, startY, endX, endY) = bbox\r\n",
        "\t\t(cX, cY) = centroid\r\n",
        "\t\tcolor = (0, 255, 0)\r\n",
        "\r\n",
        "\t\t# if the index pair exists within the violation set, then\r\n",
        "\t\t# update the color\r\n",
        "\t\tif i in violate:\r\n",
        "\t\t\tcolor = (0, 0, 255)\r\n",
        "\r\n",
        "\t\t# draw (1) a bounding box around the person and (2) the\r\n",
        "\t\t# centroid coordinates of the person,\r\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\r\n",
        "\t\tcv2.circle(frame, (cX, cY), 5, color, 1)\r\n",
        "\r\n",
        "\t# draw the total number of social distancing violations on the\r\n",
        "\t# output frame\r\n",
        "\ttext = \"Social Distancing Violations: {}\".format(len(violate))\r\n",
        "\tcv2.putText(frame, text, (10, frame.shape[0] - 25),\r\n",
        "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\r\n",
        "\r\n",
        "\t# check to see if the output frame should be displayed to our\r\n",
        "\t# screen\r\n",
        "\tif args[\"display\"] > 0:\r\n",
        "\t\t# show the output frame\r\n",
        "\t\tcv2_imshow(frame)\r\n",
        "\t\tkey = cv2.waitKey(1) & 0xFF\r\n",
        "\r\n",
        "\t\t# if the `q` key was pressed, break from the loop\r\n",
        "\t\tif key == ord(\"q\"):\r\n",
        "\t\t\tbreak\r\n",
        "\r\n",
        "\t# if an output video file path has been supplied and the video\r\n",
        "\t# writer has not been initialized, do so now\r\n",
        "\tif args[\"output\"] != \"\" and writer is None:\r\n",
        "\t\t# initialize our video writer\r\n",
        "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\r\n",
        "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 25,\r\n",
        "\t\t\t(frame.shape[1], frame.shape[0]), True)\r\n",
        "\r\n",
        "\t# if the video writer is not None, write the frame to the output\r\n",
        "\t# video file\r\n",
        "\tif writer is not None:\r\n",
        "\t\twriter.write(frame)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}